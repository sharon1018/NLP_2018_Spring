{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab05\n",
    "### Use Lesk algorighmn to classify words into right category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesk Algorithm – Disambiguating EVP entries to WordNet <br>\n",
    "Recall that LA relies on the number of shared words to determine the intended sense of a\n",
    "given context. It is not uncommon to have a tie with two or more senses having the same\n",
    "number of shared words with the context. In order to break the tie, people have used\n",
    "the weight such as idf (inverse document frequency) of word to break a tie and even to\n",
    "improve the performance. Therefore, in the training phrase (preprocessing), we read the\n",
    "dataset file and compute the following information:\n",
    "- Document frequency (df): the number of word sense categories where a certain\n",
    "word appears in. For instance, the word money appears in 80 categories including\n",
    "get.v.01. Let D be the total number of categories. Then id f = d f /D.\n",
    "- Within document term frequency: For each defining/example word, compute the\n",
    "WordNet sense categories the word appears in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF: 指某個字詞在該檔案中出現的次數/該檔案所有的字數量\n",
    "# IDF: df取倒數後對其做log10\n",
    "# - tf-idf: tf * idf\n",
    "## Step 1 : 計算senseDef裡的TF-IDF\n",
    "# - tf ：在字詞在各類 wncat 出現的次數/各類 wncat所有的字詞數量\n",
    "# - df ：在字詞出現在幾個 wncat 中/ wncat類別總數\n",
    "# - idf：df 倒數取log10\n",
    "## Step 2: split data into 9:1 and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import wordnet as wn\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "import nltk, random\n",
    "\n",
    "# 所有feature都分割成一個字\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "    \n",
    "TF = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "DF = defaultdict(lambda: [])\n",
    "wncat_count = defaultdict(lambda: 0)\n",
    "\n",
    "#詞性簡寫\n",
    "def wnTag(pos): return {'noun': 'n', 'verb': 'v', 'adjective': 'a', 'adverb': 'r'}[pos]\n",
    "\n",
    "def isHead(head, word, tag):\n",
    "    try:\n",
    "        return lmtzr.lemmatize(word, tag) == head  #詞型還原\n",
    "    except:\n",
    "        return False\n",
    "training = [line.strip().split('\\t') for line in open(r'C:/Users/asus/Downloads/nlp/Lab5/wn.in.evp.cat.txt', 'r', encoding = 'utf8') if line.strip() != '' ]\n",
    "for wnid, wncat, senseDef, target in training:\n",
    "    head, pos = wnid.split('-')[:2] #get單字 詞性\n",
    "    for word in words(senseDef): #把所有單字切割\n",
    "        if word != head and not isHead(head, word, pos): #單字或單字原型不存在時\n",
    "            TF[word][wncat] += 1 #[單字原型][類別answer]+1     #該字詞在每一類出現次數\n",
    "            DF[word] += [] if wncat in DF[word] else [wncat] #該字詞出現在那些類別\n",
    "            wncat_count[wncat]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF of (parking, get_rid_of.v.01) is -0.010101166546613746\n",
      "TFIDF of (lot, get_rid_of.v.01) is -0.010101166546613746\n",
      "feature[0]:  ({'forsake': -0.004819928076979694, 'old': -0.11958360613925677, 'in': -0.09506174447760563, 'abandon': -0.0, 'car': -0.03928251378163691, 'leave': -0.015720270738149425, 'we': -0.04687165711478301, 'parking': -0.010101166546613746, 'the': -0.230895736715218, 'behind': -0.016071399781723568, 'empty': -0.011577162612865805, 'abandoned': -0.0, 'lot': -0.016182463266751875}, 'get_rid_of.v.01')\n"
     ]
    }
   ],
   "source": [
    "##TFIDF \n",
    "# tf ：字詞在各類 wncat 出現的次數/各類 wncat所有的字詞數量\n",
    "# df ：字詞出現在幾個 wncat中/ wncat類別總數  \n",
    "# idf：df 倒數取log10\n",
    "import math\n",
    "def tfidf(word, wncat):\n",
    "    tf = TF[word][wncat]/wncat_count[wncat]\n",
    "    df = (len(DF[word]))+1/len(wncat_count)\n",
    "    idf = math.log10(1/df)\n",
    "    return tf*idf\n",
    "## testing\n",
    "print(\"TFIDF of (parking, get_rid_of.v.01) is\", tfidf(\"parking\", \"get_rid_of.v.01\"))\n",
    "print(\"TFIDF of (lot, get_rid_of.v.01) is\", tfidf(\"parking\", \"get_rid_of.v.01\"))  \n",
    "\n",
    "##取得每個字的TF-IDF\n",
    "#將training每一行的第二個(senseDef)切割, 計算每個字的TFIDF\n",
    "#{'cucumber': -0.002007382871432638, 'shaped': -0.010034365456100688...}\n",
    "def feature_format(line):\n",
    "    feature_dict={}\n",
    "    for word in words(line[2]): \n",
    "        feature_dict.update({word: tfidf(word, line[1])}) #format to dictionary\n",
    "    return (feature_dict, line[1]) \n",
    "\n",
    "## Get feature\n",
    "feature = [feature_format(line) for line in training]\n",
    "print(\"feature[0]: \", feature[0])  #多個字詞會對應到一個分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, random\n",
    "from nltk.probability import DictionaryProbDist as D  \n",
    "from nltk.classify import SklearnClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# split the feature set into 9:1\n",
    "split_ratio = int(len(feature)*9/10)\n",
    "train, test = feature[:split_ratio], feature[split_ratio:]\n",
    "\n",
    "# train with SKLearn\n",
    "sklearn_classifier = SklearnClassifier(LogisticRegression(C=10e5)).train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Sklearn accuracy=====\n",
      "0.5344827586206896\n"
     ]
    }
   ],
   "source": [
    "print(\"=====Sklearn accuracy=====\")\n",
    "print(nltk.classify.accuracy(sklearn_classifier, test))  #sklearn accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1240\n",
      "=======Accuracy======\n",
      "0.5344827586206896\n"
     ]
    }
   ],
   "source": [
    "## todo\n",
    "## compare the answer and the predict result\n",
    "# 剩下的10% test計算correct的次數\n",
    "# accuracy= correct/number of test\n",
    "\n",
    "correct=0\n",
    "for test_feature, result in test:\n",
    "    #用 test_feature預測結果 看跟result是否相同\n",
    "    chk_result = sklearn_classifier.prob_classify(test_feature)._prob_dict\n",
    "    # 機率最高的最為結果\n",
    "    chk_result = sorted(chk_result.items(), key= lambda x: -x[1])[0][0]\n",
    "    if result == chk_result:\n",
    "        correct= correct+1  # count correct\n",
    "print(correct)\n",
    "print(\"=======Accuracy======\")\n",
    "print(correct/ len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(rank.items(), key=lambda d: d[1], reverse=True)[0][0])\n",
    "\n",
    "test_len = len(test)\n",
    "correct = 0\n",
    "for feature, answer in test:\n",
    "    candidates = sklearn_classifier.prob_classify(feature)._prob_dict\n",
    "    predict = str(sorted(candidates.items(), key=lambda d: d[1], reverse=True)[0][0])\n",
    "    if answer == predict:\n",
    "        correct += 1\n",
    "\n",
    "print('accurency is',correct/test_len)\n",
    "\n",
    "import nltk\n",
    "nltk.classify.accuracy(sklearn_classifier, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
