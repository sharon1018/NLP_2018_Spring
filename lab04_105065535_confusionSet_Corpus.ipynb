{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab04\n",
    "### Correcting spelling errors in context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format:\n",
    "- Error\n",
    "- Candidates\n",
    "- Correstion\n",
    "- Wrong -> Correct\n",
    "- hits(accumulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "'''Word Probability'''\n",
    "def words(text): \n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "count_word = Counter(words(open('C:/Users/asus/Downloads/nlp/big.txt').read()))\n",
    "Nw = sum(count_word.values())\n",
    "Pdist = {word: float(count)/Nw for word, count in count_word.items()}\n",
    "\n",
    "def Pw(word): \n",
    "    return Pdist[word] if word in Pdist else 10/10**len(word)/Nw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1586\n"
     ]
    }
   ],
   "source": [
    "'''Channel Probability'''\n",
    "from collections import Counter\n",
    "with open('C:/Users/asus/Downloads/nlp/count_1edit.txt' ,encoding='UTF-8-sig') as f:\n",
    "    line = f.readlines()\n",
    "    \n",
    "\"\"\"Put every line in list\"\"\"\n",
    "line_arr=[]\n",
    "for i in range(len(line)):\n",
    "     if len(line[i])>=2:\n",
    "        line[i]= line[i][:-1].split('\\t')\n",
    "        line_arr.append(line[i])\n",
    "        \n",
    "\"\"\"Get key list\"\"\"\n",
    "k_arr=[]\n",
    "for i in range(len(line_arr)):\n",
    "        k_arr.append(tuple(line_arr[i][0].split('|')))\n",
    "\n",
    "\"\"\"Get value list\"\"\"\n",
    "v_arr=[]\n",
    "for i in range(len(line_arr)):\n",
    "    # replace wierd words\n",
    "    if line_arr[i][-1]=='' :\n",
    "        print(i)\n",
    "    else:\n",
    "        v_arr.append(int(line_arr[i][-1]))\n",
    "\n",
    "\"\"\"Zip them to dict\"\"\"\n",
    "word_dict = dict(zip(k_arr, v_arr))\n",
    "\n",
    "#(w,c)= 1,2,3次的次數\n",
    "Nr = (Counter(word_dict.values()))    #新增n0進去!\n",
    "#w,c 的次數 \n",
    "count_wc = word_dict   #新增n0進去!\n",
    "# 計算每個c出現的次數統計, 產生一個新的dict儲存\n",
    "count_c = dict()   \n",
    "for key, value in (count_wc.items()):\n",
    "    if key[1] in count_c:\n",
    "        count_c[key[1]]+=value\n",
    "    else:\n",
    "        count_c[key[1]]=value   \n",
    "Nall= sum(Nr.values())\n",
    "N0 = 26*26*26*26+2*26*26*26+26*26 - Nall   #沒有出現的組合次數\n",
    "Nr_smooth = [ N0 if r==0 else (r+1)*Nr[r+1] / Nr[r] for r in range(11) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pedit(w, c):\n",
    "    if (w,c) not in word_dict: #r=0\n",
    "        if c in count_c:         #有在正確的list中\n",
    "            return 317/N0/count_c[c]\n",
    "        else:\n",
    "            return 0\n",
    "    elif 0<count_wc[(w,c)] <= 10:  #需要smooth的\n",
    "        r = count_wc[(w,c)]\n",
    "        return Nr_smooth[r]/count_c[c]\n",
    "    else:      #>10的\n",
    "        return count_wc[(w,c)]/count_c[c]   #Ped(w, c) = count1(w, c)/count(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_states(state):\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    (L, R, edit, prob, pedit) = state\n",
    "    all_word=[]\n",
    "\n",
    "    R0, R1 = R[0:1], R[1:]\n",
    "    if edit == 2:  \n",
    "        return [( L + R0,     R1, edit,   Pw(L+R0+R1) , pedit)]\n",
    "    else:\n",
    "        all_word.append(( L + R0,     R1, edit,   Pw(L+R0+R1) , pedit))            #noedit\n",
    "        all_word.append(( L ,         R1 ,edit+1, Pw(L+R1),     pedit* Pedit(L[-1:]+R0,L[-1:]) ))  #delete\n",
    "        all_word.append(( L+ R1[0], R0+R1[1:], edit+1,Pw(L+ R1[0]+ R0+R1[1:]),pedit* Pedit(R0+R1[0],R1[0]+R0))\n",
    "                        if len(R0)>1 else (L+R1 , R0, edit+1, Pw(L+R0+R1),    pedit*Pedit(R0+R1, R1+R0))) #transpose\n",
    "        for c in letters:\n",
    "            all_word.append((L + c ,    R1, edit+1, Pw(L+c+R1),    pedit*Pedit(R0, c) )) #replace\n",
    "            all_word.append((L + R0 + c,R1, edit+1, Pw(L+R0+c+R1), pedit*Pedit(R0, R0+c) )) #insert      \n",
    "        return all_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" small edit and large probability, 最後取機率大的 \"\"\"\n",
    "\n",
    "def correction(word):\n",
    "    states = [ (\"\", word, 0, 0, 1) ]\n",
    "    for i in range(len(word)):\n",
    "        states = [ states for state in states for states in next_states(state)]\n",
    "        \n",
    "    temp = defaultdict(list)\n",
    "    for state in states:\n",
    "        L, R, edits, pw, pedit = state\n",
    "        temp[L+R].append(state)  #L+R相同的選state裡edit最小的\n",
    "    states = [min(substates, key=lambda x: (x[2],x[3]*x[4]*(-1))) for wd, substates in temp.items()]    \n",
    "## state[2]==0的取出 sort\n",
    "    states0 = [state for state in states if states[2]==0]\n",
    "    states0 = sorted(states0, key=lambda x: -1*x[3]*x[4])    \n",
    "## state[2]!=0的另外分類    \n",
    "    states_ = [state for state in states if states[2]!=0]\n",
    "    states_ = sorted(states_, key=lambda x: -1*x[3]*x[4])\n",
    "    \n",
    "    states = (states0 + states_)[:500]\n",
    "    return sorted(states, key=lambda x: ((-1)*x[3]*x[4],(-1)*x[3]))[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/asus/Downloads/nlp/Lab4/lab4.test.1.txt' ,encoding='UTF-8-sig') as f:\n",
    "    line = f.readlines()\n",
    "with open('C:/Users/asus/Downloads/nlp/Lab4/lab4.test.1.txt' ,encoding='UTF-8-sig') as f:\n",
    "    line2 = f.readlines()\n",
    "with open('C:/Users/asus/Downloads/nlp/Lab4/lab4.confusables.txt' ,encoding='UTF-8-sig') as f:\n",
    "    confusable = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"deal with cinfusable.txt\"\"\"\n",
    "for i in range(len(confusable)):\n",
    "    confusable[i] =confusable[i].split('\\t')\n",
    "    confusable[i][1] =confusable[i][1].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"deal with test1.txt\"\"\" \n",
    "# split into single word\n",
    "for i in range(len(line)):\n",
    "    line[i] = line[i].split('\\t')\n",
    "    line[i][1] = line[i][1].replace('\\n', '')\n",
    "for i in range(len(line)):\n",
    "    for j in range(2):\n",
    "        line[i][j] = line[i][j].split(' ')\n",
    "\"\"\" deal with test1.txt \"\"\"\n",
    "# split into sentence\n",
    "for i in range(len(line2)):\n",
    "    line2[i] = line2[i].split('\\t')\n",
    "    line2[i][1] = line2[i][1].replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' match with confusable list'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" match with confusable list\"\"\"\n",
    "# hit = 0\n",
    "# for n in range(len(line)):\n",
    "#     for j in range(len(line[n][0])):\n",
    "#         for i in range(len(confusable)):\n",
    "#             #比對confusable, 若存在就用confusable[i][1][0]的取代\n",
    "#             if line[n][0][j] == confusable[i][0]: \n",
    "#                 temp = line[n][0][:-1]\n",
    "#                 line[n][0][j] = line[n][0][j].replace(line[n][0][j], confusable[i][1][0])\n",
    "# #                 print(\"correct: \", line[n][0][j], \"wrong: \",confusable[i][0] )\n",
    "#                 if line[n][0][:-1] == line[n][1]:\n",
    "#                     hit = hit +1\n",
    "#                     print(\"Error: \", line[n][0][j])\n",
    "#                     print(\"Candidate: \", confusable[i][0])\n",
    "#                     print(\"Correction: \", confusable[i][0])\n",
    "#                     print(temp, \" --> \" ,line[n][0])\n",
    "#                     print(\"Hits = \", hit)\n",
    "#                     print()\n",
    "# #                     print(confusable[i][0],line[n][0][j], line[n][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'the', 'brake', 'is', 'finished']\n",
      "0\n",
      "when the brake\t7307.0\n",
      "1\n",
      "the brake is\t10569.0\n",
      "2\n",
      "not found\n"
     ]
    }
   ],
   "source": [
    "\"\"\"NetSpeak API\"\"\"\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import requests\n",
    "\n",
    "API_URL = \"http://api.netspeak.org/netspeak3/search?query=%s\"\n",
    "class NetSpeak:\n",
    "    def __init__(self):\n",
    "        self.headers = {'User-Agent': 'Mozilla/5.0 (compatible; MSIE 5.5; Windows NT)'}\n",
    "        self.page = None\n",
    "\n",
    "    def __getPageContent(self, url):\n",
    "        return requests.get(url, headers=self.headers).text\n",
    "        # return self.opener.open(url).read()\n",
    "\n",
    "    def __rolling(self, url, maxfreq=None):\n",
    "        if maxfreq:\n",
    "            webdata = self.__getPageContent(url + \"&maxfreq=%s\" % maxfreq)\n",
    "        else:\n",
    "            webdata = self.__getPageContent(url)\n",
    "        if webdata:\n",
    "            # webdata = webdata.decode('utf-8')\n",
    "            results = [data.split('\\t') for data in webdata.splitlines()]\n",
    "            results = [(data[2], float(data[1])) for data in results]\n",
    "            lastFreq = int(results[-1][1])\n",
    "            if lastFreq != maxfreq:\n",
    "                return results + self.__rolling(url, lastFreq)\n",
    "            else:\n",
    "                return []\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def search(self, query):\n",
    "        queries = query.split()\n",
    "        new_query = []\n",
    "        for token in queries:\n",
    "            if token.count('|') > 0: \n",
    "                new_query.append('[+{0}+]'.format('+'.join(token.split('|'))))\n",
    "            elif token == '*':\n",
    "                new_query.append('?')\n",
    "            else:\n",
    "                new_query.append(token)\n",
    "        new_query = '+'.join(new_query)\n",
    "        url = API_URL % (new_query.replace(' ', '+'))\n",
    "        return self.__rolling(url)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    SE = NetSpeak()\n",
    "    # Search_Result = SE.search(\"approach that *\")\n",
    "    # print Search_Result\n",
    "    # at * time\n",
    "    ## for loop\n",
    "    test = 'when the brake is finished'.split()\n",
    "    print(test)\n",
    "    # test = '? is finished'\n",
    "    # test = 'brake is finished'\n",
    "\n",
    "    for i in range(len(test) - 2):\n",
    "        print(i)\n",
    "        res = SE.search(' '.join(test[i:i + 3]))\n",
    "        if res:\n",
    "            print('\\n'.join('\\t'.join([str(y) for y in x]) for x in res))\n",
    "        else:\n",
    "            print('not found')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count correction:  0\n",
      "Error:  strang\n",
      "Candidate:  ['strange', 'strong', 'staring', 'sprang', 'string']\n",
      "Correction:  strange\n",
      "I felt very strang   -->  I felt very strange\n",
      "Hits =  1\n",
      "\n",
      "count correction:  2\n",
      "Error:  brack\n",
      "Candidate:  ['black', 'back', 'branch', 'brick', 'break']\n",
      "Correction:  back\n",
      "when the brack was finished  -->  when the back was finished\n",
      "Hits =  1\n",
      "\n",
      "count correction:  3\n",
      "Error:  weanter\n",
      "Candidate:  ['winter', 'water', 'weather', 'wanted', 'wander']\n",
      "Correction:  water\n",
      "in the weanter when it was snowing  -->  in the water when it was snowing\n",
      "Hits =  1\n",
      "\n",
      "count correction:  4\n",
      "Error:  gost\n",
      "Candidate:  ['just', 'most', 'ghost', 'got', 'goes']\n",
      "Correction:  most\n",
      "I thought it was a gost   -->  I thought it was a most\n",
      "Hits =  1\n",
      "\n",
      "count correction:  6\n",
      "Error:  steped\n",
      "Candidate:  ['stepped', 'stooped', 'striped', 'stupid', 'stopped']\n",
      "Correction:  stepped\n",
      "when I first steped   -->  when I first stepped\n",
      "Hits =  2\n",
      "\n",
      "count correction:  7\n",
      "Error:  exclation\n",
      "Candidate:  ['exclusion', 'excavation', 'exaltation', 'exultation', 'exudation']\n",
      "Correction:  exclusion\n",
      "I was on an exclation   -->  I was on an exclusion\n",
      "Hits =  2\n",
      "\n",
      "count correction:  8\n",
      "Error:  noicey\n",
      "Candidate:  ['notice', 'noisy', 'noticed', 'noise', 'nicely']\n",
      "Correction:  noticed\n",
      "I noicey that I was on this thing  -->  I noticed that I was on this thing\n",
      "Hits =  3\n",
      "\n",
      "count correction:  9\n",
      "Error:  fance\n",
      "Candidate:  ['france', 'face', 'fancy', 'fence', 'fancies']\n",
      "Correction:  fence\n",
      "through the fance   -->  through the fence\n",
      "Hits =  4\n",
      "\n",
      "count correction:  10\n",
      "Error:  kille\n",
      "Candidate:  ['killed', 'kill', 'hill', 'till', 'killer']\n",
      "Correction:  kill\n",
      "the hunters kille them  -->  the hunters kill them\n",
      "Hits =  5\n",
      "\n",
      "count correction:  11\n",
      "Error:  nerrow\n",
      "Candidate:  ['narrow', 'marrow', 'morrow', 'narrows', 'sorrow']\n",
      "Correction:  narrow\n",
      "they kill birds with their nerrow   -->  they kill birds with their narrow\n",
      "Hits =  5\n",
      "\n",
      "count correction:  12\n",
      "Error:  depe\n",
      "Candidate:  ['deep', 'deeper', 'type', 'keep', 'depth']\n",
      "Correction:  deep\n",
      "make a depe hole  -->  make a deep hole\n",
      "Hits =  6\n",
      "\n",
      "count correction:  13\n",
      "Error:  gardon\n",
      "Candidate:  ['garden', 'gordon', 'gardens', 'pardon', 'carbon']\n",
      "Correction:  garden\n",
      "to tidy up his gardon   -->  to tidy up his garden\n",
      "Hits =  7\n",
      "\n",
      "count correction:  14\n",
      "Error:  belu\n",
      "Candidate:  ['below', 'bell', 'ball', 'bill', 'begun']\n",
      "Correction:  below\n",
      "the wind belu the leaves  -->  the wind below the leaves\n",
      "Hits =  7\n",
      "\n",
      "count correction:  15\n",
      "Error:  flu\n",
      "Candidate:  ['flour', 'flew', 'flow', 'foul', 'fell']\n",
      "Correction:  flew\n",
      "the birds flu up  -->  the birds flew up\n",
      "Hits =  8\n",
      "\n",
      "count correction:  16\n",
      "Error:  leavs\n",
      "Candidate:  ['leaves', 'laws', 'leave', 'least', 'leads']\n",
      "Correction:  leaves\n",
      "garden full of leavs   -->  garden full of leaves\n",
      "Hits =  9\n",
      "\n",
      "count correction:  17\n",
      "Error:  manger\n",
      "Candidate:  ['manager', 'managers', 'danger', 'manger', 'managed']\n",
      "Correction:  manager\n",
      "talk to the manger   -->  talk to the manager\n",
      "Hits =  10\n",
      "\n",
      "count correction:  18\n",
      "Error:  aero\n",
      "Candidate:  ['were', 'air', 'her', 'aaron', 'are']\n",
      "Correction:  were\n",
      "they throw a aero   -->  they throw a were\n",
      "Hits =  10\n",
      "\n",
      "count correction:  19\n",
      "Error:  ansion\n",
      "Candidate:  ['union', 'action', 'onion', 'ention', 'anton']\n",
      "Correction:  action\n",
      "an ansion method of hunting  -->  an action method of hunting\n",
      "Hits =  10\n",
      "\n",
      "count correction:  21\n",
      "Error:  noice\n",
      "Candidate:  ['noticed', 'notice', 'noise', 'notices', 'voice']\n",
      "Correction:  noise\n",
      "making any noice   -->  making any noise\n",
      "Hits =  11\n",
      "\n",
      "count correction:  22\n",
      "Error:  stright\n",
      "Candidate:  ['straight', 'starlight', 'slight', 'strict', 'fright']\n",
      "Correction:  straight\n",
      "bring it stright to the hunters  -->  bring it straight to the hunters\n",
      "Hits =  12\n",
      "\n",
      "count correction:  23\n",
      "Error:  menes\n",
      "Candidate:  ['means', 'men', 'mines', 'miners', 'miles']\n",
      "Correction:  means\n",
      "this menes a man waits  -->  this means a man waits\n",
      "Hits =  13\n",
      "\n",
      "count correction:  24\n",
      "Error:  waight\n",
      "Candidate:  ['weight', 'weighty', 'weights', 'right', 'height']\n",
      "Correction:  right\n",
      "a man waight for the animal  -->  a man right for the animal\n",
      "Hits =  13\n",
      "\n",
      "count correction:  25\n",
      "Error:  arow\n",
      "Candidate:  ['allow', 'arrow', 'arrows', 'crowd', 'how']\n",
      "Correction:  arrow\n",
      "an arow in his hand  -->  an arrow in his hand\n",
      "Hits =  14\n",
      "\n",
      "count correction:  27\n",
      "Error:  pice\n",
      "Candidate:  ['prince', 'place', 'price', 'prices', 'picked']\n",
      "Correction:  place\n",
      "a pice of bait  -->  a place of bait\n",
      "Hits =  14\n",
      "\n",
      "count correction:  28\n",
      "Error:  tarpp\n",
      "Candidate:  ['rapp', 'tap', 'karp', 'trap', 'harp']\n",
      "Correction:  rapp\n",
      "this tarpp looks like a cave  -->  this rapp looks like a cave\n",
      "Hits =  14\n",
      "\n",
      "count correction:  30\n",
      "Error:  skeard\n",
      "Candidate:  ['steward', 'seward', 'stead', 'seared', 'heard']\n",
      "Correction:  heard\n",
      "the animals are skeard   -->  the animals are heard\n",
      "Hits =  14\n",
      "\n",
      "count correction:  31\n",
      "Error:  ribbetes\n",
      "Candidate:  ['diabetes', 'ribbts', 'ribbets', 'rbbets', 'ribbetz']\n",
      "Correction:  diabetes\n",
      "the ribbetes will run  -->  the diabetes will run\n",
      "Hits =  14\n",
      "\n",
      "count correction:  32\n",
      "Error:  stons\n",
      "Candidate:  ['stones', 'stands', 'sons', 'strong', 'storms']\n",
      "Correction:  stones\n",
      "they throw stons   -->  they throw stones\n",
      "Hits =  15\n",
      "\n",
      "count correction:  33\n",
      "Error:  chaching\n",
      "Candidate:  ['catching', 'checking', 'chasing', 'teaching', 'crashing']\n",
      "Correction:  teaching\n",
      "they chaching the animals  -->  they teaching the animals\n",
      "Hits =  15\n",
      "\n",
      "count correction:  34\n",
      "Error:  handred\n",
      "Candidate:  ['hundred', 'hundreds', 'hindered', 'handled', 'handed']\n",
      "Correction:  hundred\n",
      "three handred people  -->  three hundred people\n",
      "Hits =  16"
     ]
    }
   ],
   "source": [
    "\"\"\"match with big.txt\"\"\"\n",
    "big_txt = ((open(\"C:/Users/asus/Downloads/nlp/big.txt\", encoding='utf-8').read()))\n",
    "big_txt = big_txt.split()\n",
    "hit=0\n",
    "for i in range(len(line)): #修改前20筆\n",
    "    temp=[]\n",
    "    count=0\n",
    "    for j in range(len(line[i][0])-1):\n",
    "        if line[i][0][j] not in big_txt:\n",
    "            word = line[i][0][j]\n",
    "            ori_sent= ' '.join(line[i][0])  \n",
    "            if line[i][0][j+1] == '':  #字在最後一個\n",
    "                a= line[i][0][j-2]+ \" \"+line[i][0][j-1]+\" \"+correction(word)[0][0]\n",
    "                b= line[i][0][j-2]+ \" \"+line[i][0][j-1]+\" \"+correction(word)[1][0]\n",
    "                c= line[i][0][j-2]+ \" \"+line[i][0][j-1]+\" \"+correction(word)[2][0]\n",
    "                d= line[i][0][j-2]+ \" \"+line[i][0][j-1]+\" \"+correction(word)[3][0]\n",
    "                e= line[i][0][j-2]+ \" \"+line[i][0][j-1]+\" \"+correction(word)[4][0]\n",
    "                temp.append([a,b,c,d,e])  \n",
    "#                 ori_sent= ' '.join(line[i][0])\n",
    "                candiate = [correction(word)[0][0],correction(word)[1][0],correction(word)[2][0],correction(word)[3][0]\n",
    "                            ,correction(word)[4][0]]\n",
    "#                 Get the Max count = word[0]: sentence, word[1]: count       \n",
    "                correct_=[]\n",
    "                for m in range(5):\n",
    "                    if len(SE.search(((temp[0][m]).lower()))) !=0:\n",
    "                        correct_.append((SE.search(((temp[0][m]).lower()))))\n",
    "                        max_count=0\n",
    "                        for k in range(len(correct_)):\n",
    "                            if correct_[k][0][1]>max_count:  #find MAX\n",
    "                                max_count = correct_[k][0][1]\n",
    "                                max_word = correct_[k][0][0]\n",
    "                                c_word = max_word.split()[2] #????\n",
    "                                line[i][0] = line2[i][0].replace(word,c_word)\n",
    "                                edit_sent = (line[i][0])\n",
    "                    elif len(correct_)==0:  #所有組合都不在netspeak中\n",
    "                        correct_.append([(temp[0][0], int(10.0))])  #放回原本的, 次數為0\n",
    "#                         print(\"correct_\", correct_)\n",
    "                        max_word = correct_[0][0][0]\n",
    "                        c_word = max_word.split()[2] #????\n",
    "                        line[i][0] = line2[i][0].replace(word,c_word)\n",
    "                        edit_sent = (line[i][0])\n",
    "                                \n",
    "                if edit_sent[:-1] == line2[i][1] :  #最後有空白\n",
    "                    hit = hit +1     \n",
    "                print(\"count correction: \", i)\n",
    "                print(\"Error: \", word)\n",
    "                print(\"Candidate: \", candiate)\n",
    "                print(\"Correction: \",c_word) #not yet!\n",
    "                print( ori_sent , \" --> \",edit_sent[:-1])\n",
    "                print(\"Hits = \", hit)\n",
    "                print()   \n",
    "            elif j-1<0: #字在第一個\n",
    "                a= correction(word)[0][0]+\" \"+ line[i][0][j+1] +\" \"+ line[i][0][j+2] #取5個\n",
    "                b= correction(word)[1][0]+\" \"+ line[i][0][j+1] +\" \"+ line[i][0][j+2]\n",
    "                c= correction(word)[2][0]+ \" \"+line[i][0][j+1] +\" \"+ line[i][0][j+2]\n",
    "                d= correction(word)[3][0]+ \" \"+line[i][0][j+1] +\" \"+ line[i][0][j+2]\n",
    "                e= correction(word)[4][0]+ \" \"+line[i][0][j+1] +\" \"+ line[i][0][j+2]\n",
    "                temp.append([a,b,c,d,e])  \n",
    "                ori_sent= ori_sent= line[i][0]\n",
    "                candiate = [correction(word)[0][0],correction(word)[1][0],correction(word)[2][0],correction(word)[3][0]\n",
    "                            ,correction(word)[4][0]]\n",
    "                correct_=[]\n",
    "                for m in range(5):\n",
    "                    if len(SE.search(((temp[0][m]).lower()))) !=0:\n",
    "                        correct_.append((SE.search(((temp[0][m]).lower()))))\n",
    "                        max_count=0\n",
    "                        for k in range(len(correct_)):\n",
    "                            if correct_[k][0][1]>max_count:  #find MAX\n",
    "                                max_count = correct_[k][0][1]\n",
    "                                max_word = correct_[k][0][0]\n",
    "                                c_word = max_word.split()[0] #????\n",
    "                                line[i][0] = line2[i][0].replace(word,c_word)\n",
    "                                edit_sent = (line[i][0])\n",
    "                    elif len(correct_)==0:  #所有組合都不在netspeak中\n",
    "                        correct_.append([(temp[0][0], int(10.0))])  #放回原本的, 次數為0\n",
    "                        max_word = correct_[0][0][0]\n",
    "                        c_word = max_word.split()[0] #????\n",
    "                        line[i][0] = line2[i][0].replace(word,c_word)\n",
    "                        edit_sent = (line[i][0])\n",
    "                if edit_sent[:-1] == line2[i][1] :  #最後有空白\n",
    "                    hit = hit +1      \n",
    "                print(\"count correction: \", i)\n",
    "                print(\"Error: \", word)\n",
    "                print(\"Candidate: \", candiate)\n",
    "                print(\"Correction: \",c_word) #not yet!\n",
    "                print( ' '.join(ori_sent) , \" --> \",edit_sent[:-1])\n",
    "                print(\"Hits = \", hit)\n",
    "                print()   \n",
    "            else:  #字在中間\n",
    "                a= line[i][0][j-1]+\" \"+correction(word)[0][0]+\" \"+ line[i][0][j+1] #取5個\n",
    "                b= line[i][0][j-1]+\" \"+correction(word)[1][0]+\" \"+ line[i][0][j+1] \n",
    "                c= line[i][0][j-1]+\" \"+correction(word)[2][0]+ \" \"+line[i][0][j+1] \n",
    "                d= line[i][0][j-1]+\" \"+correction(word)[3][0]+ \" \"+line[i][0][j+1] \n",
    "                e= line[i][0][j-1]+\" \"+correction(word)[4][0]+ \" \"+line[i][0][j+1] \n",
    "                temp.append([a,b,c,d,e])\n",
    "                ori_sent= ori_sent= line[i][0]\n",
    "                candiate = [correction(word)[0][0],correction(word)[1][0],correction(word)[2][0],correction(word)[3][0]\n",
    "                            ,correction(word)[4][0]]\n",
    "                correct_=[]\n",
    "                for m in range(5):\n",
    "                    if len(SE.search(((temp[0][m]).lower()))) !=0:\n",
    "                        correct_.append((SE.search(((temp[0][m]).lower()))))\n",
    "#                         print(\"test\", correct_)\n",
    "                        max_count=0\n",
    "                        for k in range(len(correct_)):\n",
    "                            if correct_[k][0][1]>max_count:  #find MAX\n",
    "                                max_count = correct_[k][0][1]\n",
    "                                max_word = correct_[k][0][0]\n",
    "                                c_word = max_word.split()[1] \n",
    "                                line[i][0] = line2[i][0].replace(word,c_word)\n",
    "                                edit_sent = line[i][0]\n",
    "                    elif len(correct_)==0:  #所有組合都不在netspeak中\n",
    "                        correct_.append([(temp[0][0], int(10.0))])  #放回原本的, 次數為0\n",
    "#                         print(\"correct_\", correct_)\n",
    "                        max_word = correct_[0][0][0]\n",
    "                        c_word = max_word.split()[1] #????\n",
    "                        line[i][0] = line2[i][0].replace(word,c_word)\n",
    "                        edit_sent = (line[i][0])\n",
    "                \n",
    "                if edit_sent[:-1] == line2[i][1] :  #最後有空白\n",
    "                    hit = hit +1  \n",
    "                print(\"count correction: \", i)\n",
    "                print(\"Error: \", word)\n",
    "                print(\"Candidate: \", candiate)\n",
    "                print(\"Correction: \",c_word) #not yet!\n",
    "                print(' '.join(ori_sent)[:-1] , \" --> \",edit_sent[:-1])\n",
    "                print(\"Hits = \", hit)\n",
    "                print()   \n",
    "                break\n",
    "                \n",
    "#             count = count+1  #if 次數=長度-->整句每個字都有出現在字典裡\n",
    "#             if count == len(line[i][0])-1:   # 都有在big txt內的再放進confusion    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_[0][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hunters kill them', 139.0)\n"
     ]
    }
   ],
   "source": [
    "# find the MAX\n",
    "for i in range(2):\n",
    "    temp=0\n",
    "    if correct_[i][0][1]>temp:\n",
    "        temp = correct_[i][0][1]\n",
    "        word = correct_[i][0]\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_flow(max_count):#????\n",
    "    return max_count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"split sentence into tri-gram\"\"\"\n",
    "def trigram(text):\n",
    "    trigram_word=[]\n",
    "    trigram = (ngrams(text.split(), 3))\n",
    "    for text in trigram:\n",
    "        trigram_word.append(\" \".join(text))\n",
    "    return trigram_word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
